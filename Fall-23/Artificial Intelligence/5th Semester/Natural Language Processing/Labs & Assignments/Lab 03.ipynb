{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Lab Session 3: Word Embeddings (Word2Vec)</b></h1>\n"
      ],
      "metadata": {
        "id": "WGrKlWqIdc20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **CBOW** predicts a target word from surrounding context words.\n",
        "* **Skip-gram** predicts surrounding context words from a target word (better for rare words).\n",
        "* **Negative Sampling** replaces full softmax with a small binary classification task: push real (target, context) pairs up; push randomly sampled “negative” pairs down.\n",
        "\n",
        "**Key-terms**\n",
        "* Window size: how far to look around the center word.\n",
        "* Embedding dim: size of vector.\n",
        "* Min count: drop very rare words.\n",
        "* Subsampling: downweight very frequent words (“the”, “of”…).\n",
        "\n",
        "**Learning goals:**\n",
        "\n",
        "* Understand the distributional hypothesis and why embeddings work.\n",
        "\n",
        "* Explain Word2Vec (CBOW vs Skip-gram, window size, negative sampling).\n",
        "\n",
        "* Train Word2Vec with a library (fast path).\n",
        "\n",
        "* Implement a tiny from-scratch Skip-gram + Negative Sampling model.\n",
        "\n",
        "\n",
        "**Required tools:** Python 3.9+, basic NumPy/PyTorch, basic linear algebra (dot product, cosine)"
      ],
      "metadata": {
        "id": "YlF2XvLzdFSe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4R5SLXiYbzx",
        "outputId": "578d62e9-333e-4661-8f7c-4fd468812bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy gensim torch matplotlib scikit-learn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h2><b>1. Warm-up</b></h2>\n",
        "\n",
        "**Idea:** “You shall know a word by the company it keeps.”\n",
        "Given a toy corpus, compute a co-occurrence matrix and show that cosine-similar words share contexts.\n",
        "\n",
        "**Note:** This isn’t Word2Vec—just a warm-up to see context-based similarity."
      ],
      "metadata": {
        "id": "x4l2m7vkZyxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "corpus = [\n",
        "    \"king queen royal palace\",\n",
        "    \"man woman boy girl\",\n",
        "    \"paris france capital city\",\n",
        "    \"rome italy capital city\",\n",
        "    \"king man queen woman\",\n",
        "]\n",
        "\n",
        "tokens = [w for line in corpus for w in line.split()]\n",
        "vocab = sorted(set(tokens))\n",
        "idx = {w:i for i,w in enumerate(vocab)}\n",
        "\n",
        "window = 2\n",
        "cooc = np.zeros((len(vocab), len(vocab)), dtype=np.float32)\n",
        "\n",
        "for line in corpus:\n",
        "    ws = line.split()\n",
        "    for i, w in enumerate(ws):\n",
        "        wi = idx[w]\n",
        "        for j in range(max(0, i-window), min(len(ws), i+window+1)):\n",
        "            if i == j: continue\n",
        "            cooc[wi, idx[ws[j]]] += 1\n",
        "\n",
        "def cosine(a,b):\n",
        "    return a@b / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-9)\n",
        "\n",
        "def most_sim(word, k=3):\n",
        "    i = idx[word]\n",
        "    sims = [(vocab[j], float(cosine(cooc[i], cooc[j]))) for j in range(len(vocab)) if j != i]\n",
        "    return sorted(sims, key=lambda x: x[1], reverse=True)[:k]\n",
        "\n",
        "print(\"Similar to 'king':\", most_sim(\"king\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flgH4EkVZntB",
        "outputId": "8147b920-3828-41bc-be5d-62ca890b57cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar to 'king': [('palace', 0.8660253594734205), ('woman', 0.6172134141557665), ('royal', 0.4714044890112376)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>2. Using Library for Word2Vec</b></h2>\n",
        "Use gensim to train Word2Vec and query similar words."
      ],
      "metadata": {
        "id": "toKtRA1YbnxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "raw_text = \"\"\"\n",
        "Paris is the capital of France. Rome is the capital of Italy.\n",
        "The king and the queen visited the royal palace.\n",
        "A boy and a girl met a man and a woman in the city.\n",
        "\"\"\"\n",
        "sentences = [simple_preprocess(s) for s in raw_text.strip().split(\"\\n\")]\n",
        "\n",
        "# Skip-gram (sg=1); negative sampling (negative>0)\n",
        "model = Word2Vec(sentences,\n",
        "                 vector_size=50, window=2, min_count=1,\n",
        "                 sg=1, negative=5, epochs=200, workers=1, seed=0)\n",
        "\n",
        "print(model.wv.most_similar(\"king\", topn=5))\n",
        "print(model.wv.similarity(\"paris\", \"france\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDI_eH-xbqAk",
        "outputId": "f4d74c81-90a7-40a7-a012-6448e8d99c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('visited', 0.5827963352203369), ('the', 0.36465612053871155), ('city', 0.30424636602401733), ('royal', 0.24916714429855347), ('girl', 0.186227485537529)]\n",
            "0.20128465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>What’s happening:<h2>\n",
        "\n",
        "gensim builds vocab, samples (target, context) pairs within the window, then trains embeddings using skip-gram + neg sampling. model.wv[...] gives vectors.\n",
        "\n",
        "**Exercise A:** Change window and vector_size. Observe changes in neighbors for “king”, “paris”."
      ],
      "metadata": {
        "id": "v5HmA9vXfQCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>3. From Scratch: Skip-gram + Negative Sampling</b></h2>\n",
        "\n",
        "**3.1 Data preparation:**\n",
        "* Tokenize\n",
        "* Build vocab & frequent-word sampling table,\n",
        "* Generate training pairs (center → context) within window,\n",
        "* Draw negatives from a unigram distribution^0.75 (as in the original paper)."
      ],
      "metadata": {
        "id": "lGRwduaQlHmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random, math\n",
        "from collections import Counter\n",
        "\n",
        "text = \"king queen royal palace man woman boy girl paris france capital city rome italy capital city king man queen woman\"\n",
        "tokens = text.split()\n",
        "\n",
        "# Build vocab\n",
        "min_count = 1\n",
        "cnt = Counter(tokens)\n",
        "vocab = [w for w,c in cnt.items() if c >= min_count]\n",
        "stoi = {w:i for i,w in enumerate(vocab)}\n",
        "itos = {i:w for w,i in stoi.items()}\n",
        "ids = [stoi[w] for w in tokens]\n",
        "\n",
        "# Unigram^0.75 table for negative sampling\n",
        "pow_freq = torch.tensor([cnt[itos[i]]**0.75 for i in range(len(vocab))], dtype=torch.float)\n",
        "neg_dist = pow_freq / pow_freq.sum()\n",
        "\n",
        "def generate_pairs(ids, window=2):\n",
        "    pairs = []\n",
        "    for i, center in enumerate(ids):\n",
        "        left = max(0, i-window); right = min(len(ids), i+window+1)\n",
        "        for j in range(left, right):\n",
        "            if i==j: continue\n",
        "            pairs.append((center, ids[j]))\n",
        "    return pairs\n",
        "\n",
        "pairs = generate_pairs(ids, window=2)\n",
        "#print(f'Token:{tokens}[ID: {ids}]')\n",
        "for t,i in zip(tokens,ids):\n",
        "  print(f'Token:{t} [ID:{i}]')\n",
        "print(f'Sample pairs')\n",
        "#print(\"Sample pairs:\", pairs[:6], f\"(total {len(pairs)})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJfFihnofiTy",
        "outputId": "ceb617a0-4e71-4372-88d1-f1fde58016a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:king [ID:0]\n",
            "Token:queen [ID:1]\n",
            "Token:royal [ID:2]\n",
            "Token:palace [ID:3]\n",
            "Token:man [ID:4]\n",
            "Token:woman [ID:5]\n",
            "Token:boy [ID:6]\n",
            "Token:girl [ID:7]\n",
            "Token:paris [ID:8]\n",
            "Token:france [ID:9]\n",
            "Token:capital [ID:10]\n",
            "Token:city [ID:11]\n",
            "Token:rome [ID:12]\n",
            "Token:italy [ID:13]\n",
            "Token:capital [ID:10]\n",
            "Token:city [ID:11]\n",
            "Token:king [ID:0]\n",
            "Token:man [ID:4]\n",
            "Token:queen [ID:1]\n",
            "Token:woman [ID:5]\n",
            "Sample pairs: [(0, 1), (0, 2), (1, 0), (1, 2), (1, 3), (2, 0)] (total 74)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Model:**\n",
        "\n",
        "We maintain two embedding matrices:\n",
        "* *in_embed* for centers,\n",
        "* *out_embed* for contexts.\n",
        "\n",
        "**Loss per batch:**\n",
        "Sum of log-sigmoid for positive pairs + log-sigmoid(−score) for negatives."
      ],
      "metadata": {
        "id": "ffxUNP2WooaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramNeg(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "        self.in_embed  = torch.nn.Embedding(vocab_size, dim)\n",
        "        self.out_embed = torch.nn.Embedding(vocab_size, dim)\n",
        "        torch.nn.init.uniform_(self.in_embed.weight,  -0.5/dim, 0.5/dim)\n",
        "        torch.nn.init.zeros_(self.out_embed.weight)\n",
        "\n",
        "    def forward(self, center_ids, pos_context_ids, neg_context_ids):\n",
        "        # center: (B, D), pos: (B, D), neg: (B, K, D)\n",
        "        center = self.in_embed(center_ids)                  # [B, D]\n",
        "        pos    = self.out_embed(pos_context_ids)            # [B, D]\n",
        "        neg    = self.out_embed(neg_context_ids)            # [B, K, D]\n",
        "\n",
        "        # Positive scores: dot(center, pos)\n",
        "        pos_score = torch.sum(center * pos, dim=1)          # [B]\n",
        "        pos_loss  = torch.log(torch.sigmoid(pos_score) + 1e-9)\n",
        "\n",
        "        # Negative scores: dot(center, neg) -> want them small\n",
        "        neg_score = torch.bmm(neg, center.unsqueeze(2)).squeeze(2)  # [B, K]\n",
        "        neg_loss  = torch.log(torch.sigmoid(-neg_score) + 1e-9).sum(1)  # [B]\n",
        "\n",
        "        # Maximize pos_loss + neg_loss -> minimize negative\n",
        "        loss = -(pos_loss + neg_loss).mean()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "aPaC2h9bo1md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Training loop**\n",
        "\n",
        "* For each (center, positive_context), sample K negatives from neg_dist (avoiding the positive).\n",
        "* Optimize with Adam for a few epochs."
      ],
      "metadata": {
        "id": "ApvjRiDTpmob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "dim = 50\n",
        "K = 5\n",
        "batch_size = 32\n",
        "epochs = 200\n",
        "model = SkipGramNeg(len(vocab), dim).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "def sample_neg(batch_size, K):\n",
        "    # Multinomial draws negatives i.i.d. from neg_dist\n",
        "    return torch.multinomial(neg_dist, batch_size*K, replacement=True).view(batch_size, K)\n",
        "\n",
        "# Tiny dataset: convert pairs to tensors\n",
        "pairs_t = [(torch.tensor(c), torch.tensor(p)) for c,p in pairs]\n",
        "\n",
        "for ep in range(1, epochs+1):\n",
        "    random.shuffle(pairs_t)\n",
        "    total = 0.0\n",
        "    for i in range(0, len(pairs_t), batch_size):\n",
        "        batch = pairs_t[i:i+batch_size]\n",
        "        c_ids = torch.stack([b[0] for b in batch]).to(device)\n",
        "        p_ids = torch.stack([b[1] for b in batch]).to(device)\n",
        "        n_ids = sample_neg(len(batch), K).to(device)\n",
        "\n",
        "        loss = model(c_ids, p_ids, n_ids)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        total += float(loss)\n",
        "\n",
        "    if ep % 50 == 0:\n",
        "        print(f\"epoch {ep}: loss={total/ (len(pairs_t)/batch_size):.4f}\")\n",
        "\n",
        "# Get final input embeddings\n",
        "emb = model.in_embed.weight.detach().cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnOThmhZpFbt",
        "outputId": "e3aceab9-0453-4d27-86c7-268111b1b5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2595951285.py:27: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  total += float(loss)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 50: loss=2.6042\n",
            "epoch 100: loss=2.4678\n",
            "epoch 150: loss=2.3446\n",
            "epoch 200: loss=2.2777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What the code does (in one epoch):**\n",
        "\n",
        "We build training pairs using a sliding window; for each positive pair we draw K negatives; the model learns two embedding tables so that dot products of true (center, context) go high, and with random contexts go low. The objective is the sum of log-sigmoid terms."
      ],
      "metadata": {
        "id": "k7kMfPf_p4nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4 Inspect embeddings**\n",
        "\n",
        "Nearest neighbors by cosine similarity:"
      ],
      "metadata": {
        "id": "HcRJ_eU1qF7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "def most_sim(word, topk=5):\n",
        "    wid = stoi[word]\n",
        "    v = emb[wid]\n",
        "    sims = []\n",
        "    for i in range(len(vocab)):\n",
        "        if i == wid: continue\n",
        "        s = float(v @ emb[i] / (norm(v)*norm(emb[i]) + 1e-9))\n",
        "        sims.append((itos[i], s))\n",
        "    sims.sort(key=lambda x: x[1], reverse=True)\n",
        "    return sims[:topk]\n",
        "\n",
        "print(\"NN(king):\", most_sim(\"king\"))\n",
        "print(\"NN(paris):\", most_sim(\"paris\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA0-v-sfqOKz",
        "outputId": "78032988-3663-4cc3-bf0d-701cd92d4812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN(king): [('palace', 0.6836001783374662), ('rome', 0.5998950395818237), ('italy', 0.48061624502538514), ('royal', 0.4591892446706788), ('france', 0.3587692084619158)]\n",
            "NN(paris): [('girl', 0.6003517195244562), ('france', 0.5463207315231119), ('woman', 0.5261117139363036), ('italy', 0.4479966605957958), ('city', 0.40906032642926526)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(a,b,c, topk=3):\n",
        "    va, vb, vc = emb[stoi[a]], emb[stoi[b]], emb[stoi[c]]\n",
        "    q = vb - va + vc\n",
        "    sims = []\n",
        "    for i in range(len(vocab)):\n",
        "        w = itos[i]\n",
        "        if w in {a,b,c}: continue\n",
        "        s = float(q @ emb[i] / (norm(q)*norm(emb[i]) + 1e-9))\n",
        "        sims.append((w,s))\n",
        "    sims.sort(key=lambda x: x[1], reverse=True)\n",
        "    return sims[:topk]\n",
        "\n",
        "print(\"king:man :: ? : woman →\", analogy(\"king\",\"man\",\"woman\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19t1Xwvrzbg0",
        "outputId": "826e6f7a-bc7c-4e55-c4ac-e2d584ab16fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "king:man :: ? : woman → [('girl', 0.5089562247328274), ('royal', 0.49312329477134664), ('paris', 0.36166756567992847)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>4. Word2Vec with gensim on 20 Newsgroups (≈11k documents)</b></h2>\n",
        "\n",
        "* Load a real-world text dataset (20 Newsgroups).\n",
        "* Train skip-gram + negative sampling Word2Vec in gensim.\n",
        "* Explore nearest neighbors, word similarity, and simple analogies.\n",
        "* Save & reload the model."
      ],
      "metadata": {
        "id": "gKLL2d-TtE8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk; nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDfMpmFAwMLG",
        "outputId": "0afbe9b6-fb4e-4d47-d5fa-5c7d18a9a989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 Load & preprocess the dataset**"
      ],
      "metadata": {
        "id": "-N0342do1rWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Grab a reasonable slice (you can use subset='all' if you like)\n",
        "data = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
        "docs = data.data  # ~11k short news-like posts\n",
        "\n",
        "# Convert documents → list of tokenized sentences for Word2Vec\n",
        "# sent_tokenize splits into sentences (better training pairs).\n",
        "# simple_preprocess = lowercase + basic cleanup. No heavy NLP pipeline needed.\n",
        "sentences = []\n",
        "for doc in docs:\n",
        "    for s in sent_tokenize(doc):\n",
        "        tokens = simple_preprocess(s, min_len=2, max_len=20)  # lowercase, strip punct, keep words\n",
        "        if tokens:\n",
        "            sentences.append(tokens)\n",
        "\n",
        "print(len(docs), \"documents\")\n",
        "print(docs[0])\n",
        "print(len(sentences), \"sentences, e.g.:\", sentences[0][:12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lJ3XdjLwO3f",
        "outputId": "0a06cea7-a0ac-4442-b14e-30338fc4dc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314 documents\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "125218 sentences, e.g.: ['was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Train Word2Vec (skip-gram + negative sampling)**\n",
        "\n",
        "**Tip:** If many probes are “not in vocab,” lower min_count to 3 (slower, bigger vocab) or raise epochs."
      ],
      "metadata": {
        "id": "q94Dmbos2NQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Small, fast settings for a demo. You can increase vector_size/epochs later for better quality.\n",
        "w2v = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=100,      # embedding dim\n",
        "    window=5,             # context window\n",
        "    min_count=5,          # ignore very rare words\n",
        "    workers=4,            # CPU threads\n",
        "    sg=1,                 # 1=skip-gram, 0=CBOW\n",
        "    negative=10,          # negative samples\n",
        "    epochs=5,             # training passes (raise to 10–20 for quality)\n",
        "    sample=1e-3,          # subsampling for frequent words\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "Q-mVJefmw5f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What this does (brief):**\n",
        "\n",
        "Skip-gram creates (center → context) pairs inside a sliding window and learns vectors so true context pairs score high and random pairs score low (negative sampling)."
      ],
      "metadata": {
        "id": "yuprUDnI2YPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Short exercises (Make changes to model)</b></h2>\n",
        "\n",
        "1. Window size study: Train with window ∈ {1,2,5}. Report nearest neighbors of capital and king. What changes? Why?\n",
        "\n",
        "2. Negative samples K: Try K ∈ {2,5,15}. How do loss and neighbors change?\n",
        "\n",
        "3. Rare words: Add new sentences that use palace and royal more often. Does “palace” move closer to “royal/king/queen”?\n",
        "\n",
        "4. CBOW vs Skip-gram (challenge): Modify the training pairs so context→target (average context embeddings to predict center). Compare.\n",
        "\n",
        "5. Subsampling (extra): Downsample frequent tokens (e.g., “the”, “is”). Observe speed/quality."
      ],
      "metadata": {
        "id": "lqjbi47-zjJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Quick exploration**"
      ],
      "metadata": {
        "id": "RKUtF2GG2wJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv = w2v.wv  # keyed vectors\n",
        "\n",
        "def try_neighbors(word, topn=10):\n",
        "    if word in wv:\n",
        "        print(f\"\\nMost similar to '{word}':\")\n",
        "        for w,score in wv.most_similar(word, topn=topn):\n",
        "            print(f\"  {w:15s}  {score:.3f}\")\n",
        "    else:\n",
        "        print(f\"'{word}' not in vocab (try a more frequent term).\")\n",
        "\n",
        "# Try domain-relevant words commonly found in 20NG:\n",
        "for probe in [\"computer\", \"windows\", \"mac\", \"graphics\", \"game\",\n",
        "              \"religion\", \"god\", \"church\", \"hockey\", \"space\",\n",
        "              \"science\", \"medical\", \"gun\", \"government\"]:\n",
        "    try_neighbors(probe)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfEEzYB2w6EW",
        "outputId": "4bfbf189-0d9a-4f66-ff9d-d1ec53959542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most similar to 'computer':\n",
            "  engineering      0.695\n",
            "  bulletin         0.691\n",
            "  shopper          0.690\n",
            "  electronic       0.686\n",
            "  vlsi             0.680\n",
            "  peripheral       0.674\n",
            "  calculator       0.674\n",
            "  silicon          0.665\n",
            "  isdn             0.664\n",
            "  architecture     0.661\n",
            "\n",
            "Most similar to 'windows':\n",
            "  apps             0.820\n",
            "  microsoft        0.804\n",
            "  workgroups       0.783\n",
            "  excel            0.779\n",
            "  emm              0.753\n",
            "  os               0.750\n",
            "  compiler         0.750\n",
            "  paradox          0.750\n",
            "  povray           0.750\n",
            "  sdk              0.749\n",
            "\n",
            "Most similar to 'mac':\n",
            "  amiga            0.769\n",
            "  atari            0.758\n",
            "  clone            0.755\n",
            "  iisi             0.751\n",
            "  roms             0.738\n",
            "  keyboards        0.734\n",
            "  netware          0.734\n",
            "  iici             0.734\n",
            "  trident          0.733\n",
            "  compaq           0.733\n",
            "\n",
            "Most similar to 'graphics':\n",
            "  gems             0.761\n",
            "  animation        0.761\n",
            "  assembler        0.753\n",
            "  multimedia       0.753\n",
            "  raytracing       0.751\n",
            "  cad              0.750\n",
            "  radiosity        0.742\n",
            "  toolkits         0.737\n",
            "  gui              0.736\n",
            "  bibliography     0.726\n",
            "\n",
            "Most similar to 'game':\n",
            "  games            0.823\n",
            "  goalie           0.777\n",
            "  stanley          0.771\n",
            "  playoffs         0.761\n",
            "  shutouts         0.759\n",
            "  wins             0.755\n",
            "  goalies          0.749\n",
            "  inning           0.748\n",
            "  streak           0.748\n",
            "  playoff          0.746\n",
            "\n",
            "Most similar to 'religion':\n",
            "  islam            0.839\n",
            "  theology         0.817\n",
            "  doctrines        0.807\n",
            "  censorship       0.785\n",
            "  sexuality        0.785\n",
            "  christianity     0.776\n",
            "  teaching         0.774\n",
            "  religious        0.773\n",
            "  morals           0.772\n",
            "  conspiracy       0.772\n",
            "\n",
            "Most similar to 'god':\n",
            "  christ           0.825\n",
            "  lord             0.817\n",
            "  allah            0.806\n",
            "  faith            0.802\n",
            "  spirit           0.801\n",
            "  jesus            0.793\n",
            "  truth            0.790\n",
            "  salvation        0.782\n",
            "  eternal          0.782\n",
            "  mercy            0.771\n",
            "\n",
            "Most similar to 'church':\n",
            "  liturgy          0.808\n",
            "  coptic           0.799\n",
            "  doctrine         0.793\n",
            "  doctrines        0.783\n",
            "  lds              0.780\n",
            "  theology         0.779\n",
            "  creed            0.777\n",
            "  christian        0.764\n",
            "  teachings        0.763\n",
            "  incarnation      0.761\n",
            "\n",
            "Most similar to 'hockey':\n",
            "  basketball       0.792\n",
            "  nhl              0.774\n",
            "  playoff          0.769\n",
            "  baseball         0.768\n",
            "  junior           0.765\n",
            "  football         0.758\n",
            "  regulars         0.753\n",
            "  championships    0.745\n",
            "  scores           0.744\n",
            "  soccer           0.735\n",
            "\n",
            "Most similar to 'space':\n",
            "  shuttle          0.726\n",
            "  hubble           0.633\n",
            "  planetary        0.630\n",
            "  nasa             0.626\n",
            "  jsc              0.620\n",
            "  manned           0.619\n",
            "  lunar            0.614\n",
            "  schedules        0.611\n",
            "  astronaut        0.610\n",
            "  launch           0.609\n",
            "\n",
            "Most similar to 'science':\n",
            "  fiction          0.786\n",
            "  unified          0.748\n",
            "  scientist        0.741\n",
            "  mathematics      0.738\n",
            "  scientific       0.732\n",
            "  biological       0.729\n",
            "  philosophy       0.727\n",
            "  psychology       0.714\n",
            "  cryptology       0.712\n",
            "  engineering      0.711\n",
            "\n",
            "Most similar to 'medical':\n",
            "  hicnet           0.775\n",
            "  newsletter       0.741\n",
            "  clinical         0.729\n",
            "  aids             0.718\n",
            "  hospitals        0.701\n",
            "  nutrition        0.696\n",
            "  journals         0.695\n",
            "  cancer           0.680\n",
            "  expertise        0.673\n",
            "  trials           0.673\n",
            "\n",
            "Most similar to 'gun':\n",
            "  handgun          0.717\n",
            "  defenses         0.687\n",
            "  firearms         0.685\n",
            "  crime            0.678\n",
            "  ownership        0.669\n",
            "  legislation      0.664\n",
            "  criminal         0.662\n",
            "  guns             0.659\n",
            "  stricter         0.656\n",
            "  owning           0.652\n",
            "\n",
            "Most similar to 'government':\n",
            "  nsa              0.738\n",
            "  actively         0.724\n",
            "  governments      0.722\n",
            "  wiretapping      0.717\n",
            "  govt             0.716\n",
            "  cooperation      0.709\n",
            "  agencies         0.708\n",
            "  advocates        0.707\n",
            "  tyranny          0.701\n",
            "  preserve         0.695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.4 Word similarity & analogies**"
      ],
      "metadata": {
        "id": "3NrqocxV3w4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sim(a, b):\n",
        "    if a in wv and b in wv:\n",
        "        print(f\"similarity({a}, {b}) = {wv.similarity(a, b):.3f}\")\n",
        "    else:\n",
        "        print(f\"Missing: {a if a not in wv else ''} {b if b not in wv else ''}\")\n",
        "\n",
        "# Similarity checks\n",
        "sim(\"computer\", \"graphics\")\n",
        "sim(\"hockey\", \"game\")\n",
        "sim(\"religion\", \"church\")\n",
        "sim(\"space\", \"nasa\")\n",
        "\n",
        "# Analogy: a : b :: c : ?\n",
        "# Example: \"king - man + woman ≈ queen\" often won’t work perfectly on this dataset—try topical ones:\n",
        "def analogy(a, b, c, topn=5):\n",
        "    if all(w in wv for w in [a,b,c]):\n",
        "        print(f\"\\nAnalogy: {a} : {b} :: {c} : ?\")\n",
        "        for w,score in wv.most_similar(positive=[b, c], negative=[a], topn=topn):\n",
        "            print(f\"  {w:15s}  {score:.3f}\")\n",
        "    else:\n",
        "        print(\"Analogy words missing from vocab.\")\n",
        "\n",
        "analogy(\"computer\", \"windows\", \"mac\")     # OS/software vibe\n",
        "analogy(\"space\", \"nasa\", \"astronomy\")     # topical\n",
        "analogy(\"hockey\", \"game\", \"team\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMb68BkZxq-9",
        "outputId": "50d826ec-9c60-4530-86cf-d122be73e15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity(computer, graphics) = 0.503\n",
            "similarity(hockey, game) = 0.635\n",
            "similarity(religion, church) = 0.700\n",
            "similarity(space, nasa) = 0.626\n",
            "\n",
            "Analogy: computer : windows :: mac : ?\n",
            "  os               0.689\n",
            "  apps             0.651\n",
            "  nt               0.629\n",
            "  linux            0.614\n",
            "  exe              0.611\n",
            "\n",
            "Analogy: space : nasa :: astronomy : ?\n",
            "  jpl              0.818\n",
            "  dryden           0.751\n",
            "  larc             0.747\n",
            "  lerc             0.747\n",
            "  spacelink        0.744\n",
            "\n",
            "Analogy: hockey : game :: team : ?\n",
            "  scored           0.672\n",
            "  score            0.668\n",
            "  games            0.665\n",
            "  inning           0.659\n",
            "  wins             0.645\n"
          ]
        }
      ]
    }
  ]
}